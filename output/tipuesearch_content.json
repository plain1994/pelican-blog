{"pages":[{"url":"http://blog.tomgou.xyz/shi-yong-pycharmpei-zhi-sparkde-pythonkai-fa-huan-jing.html","text":"在本地搭建好Spark 1.6.0后，除了使用spark-submit提交Python程序外，我们可以使用PyCharm这个IDE在本地进行开发调试,提升我们的开发效率。配置过程也十分简单，在stackoverflow上搜索到的。同时，IntelliJ IDEA加入Python插件后也可以使用Python开发Spark程序，配置步骤一致。转载请注明博客原文地址： http://blog.tomgou.xyz/shi-yong-pycharmpei-zhi-sparkde-pythonkai-fa-huan-jing.html 0.安装PyCharm和py4j 我的系统环境（Ubuntu 14.04.4 LTS） 下载安装最新版本的PyCharm，官网地址： https://www.jetbrains.com/pycharm/download/ 。 安装步骤： Unpack the pycharm-5.0.4.tar.gz using the following command: tar xfz pycharm-5.0.4.tar.gz Run pycharm.sh from the bin subdirectory 安装py4j： $ sudo pip install py4j 1.配置Pycharm 打开PyCharm，创建一个Project。 然后选择\"Run\" ->\"Edit Configurations\" ->\"Environment variables\" 增加SPARK_HOME目录与PYTHONPATH目录。 - SPARK_HOME:Spark安装目录 - PYTHONPATH:Spark安装目录下的Python目录 2.测试Pycharm 运行一个小的Spark程序看看： \"\"\"SimpleApp\"\"\" from pyspark import SparkContext logFile = \"/home/tom/spark-1.6.0/README.md\" sc = SparkContext ( \"local\" , \"Simple App\" ) logData = sc . textFile ( logFile ) . cache () numAs = logData . filter ( lambda s : 'a' in s ) . count () numBs = logData . filter ( lambda s : 'b' in s ) . count () print ( \"Lines with a: %i , lines with b: %i \" % ( numAs , numBs )) 运行结果： Lines with a: 58, lines with b: 26","tags":"Blogs","title":"使用PyCharm配置Spark的Python开发环境"},{"url":"http://blog.tomgou.xyz/shi-yong-intellij-ideapei-zhi-sparkying-yong-kai-fa-huan-jing-ji-yuan-ma-yue-du-huan-jing.html","text":"在本地搭建好Spark 1.6.0后，除了使用官方文档中的sbt命令打包，spark-submit提交程序外，我们可以使用IntelliJ IDEA这个IDE在本地进行开发调试，之后再将作业提交到集群生产环境中运行，使用IDE可以提升我们的开发效率。转载请注明博客原文地址： http://blog.tomgou.xyz/shi-yong-intellij-ideapei-zhi-sparkying-yong-kai-fa-huan-jing-ji-yuan-ma-yue-du-huan-jing.html 0.安装IntelliJ IDEA 我的系统环境（Ubuntu 14.04.4 LTS） 下载最新版本的IntelliJ IDEA，官网地址： https://www.jetbrains.com/idea/download/ 。 最新版本的IntelliJ IDEA支持新建SBT工程，安装scala插件。 安装步骤： Unpack the idea idea-15.0.4.tar.gz file using the following command: tar xfz idea-15.0.4.tar.gz Run idea.sh from the bin subdirectory. 记得在IntelliJ IDEA的\"Configure\"菜单中，选择\"Plugins\"，安装\"Scala\"插件。 1.以本地local模式运行Spark程序 1）创建\"New Project\"，选择\"Scala\"。\"Project SDK\"选择JDK目录，\"Scala SDK\"选择Scala目录。 2）选择菜单中的\"File\" ->\"Project Structure\" ->\"libraries\" ->+\"java\"，导入Spark安装目录 /home/tom/spark-1.6.0/lib 下的\" spark-assembly-1.6.0-hadoop2.6.0.jar \"。 3）运行Scala示例程序SparkPi： Spark安装目录的examples目录下，可以找到Scala编写的示例程序 SparkPi.scala ，该程序计算Pi值并输出。 在Project的main目录下新建 SparkPitest.scala ，复制Spark示例程序代码。 选择菜单中的\"Run\" ->\"Edit Configurations\"，修改\"Main class\"和\"VM options\"。 运行结果： 注意： 在我最初运行Spark的测试程序SparkPi时，点击运行，出现了如下错误： Exception in thread \"main\" org.apache.spark.SparkException: A master URL must be set in your configuration 从提示中可以看出找不到程序运行的master，此时需要配置环境变量。 搜索引擎查询错误后，了解到传递给spark的master url可以有如下几种，具体可以查看Spark官方文档： local 本地单线程 local[K] 本地多线程（指定K个内核） local[*] 本地多线程（指定所有可用内核） spark://HOST:PORT 连接到指定的 Spark standalone cluster master，需要指定端口。 mesos://HOST:PORT 连接到指定的 Mesos 集群，需要指定端口。 yarn-client客户端模式 连接到 YARN 集群。需要配置 HADOOP_CONF_DIR。 yarn-cluster集群模式 连接到 YARN 集群。需要配置 HADOOP_CONF_DIR。 在VM options中输入\" -Dspark.master=local \"，指示本程序本地单线程运行。 2.生成jar包提交到集群 1）和本地local模式运行Spark相同，我们建立起project。 2）选择菜单中的\"File\" ->\"Project Structure\" ->\"Artifact\" ->\"jar\" ->\"From Modules with dependencies\"，之后选择Main Class和输出jar的Directory。 3）在主菜单选择\"Build\" ->\"Build Artifact\"，编译生成jar包。 4）将jar包使用 spark-submit 提交： $SPARK_HOME/bin/spark-submit --class \"SimpleApp\" --master local [ 4 ] simple.jar 3.配置Spark源码阅读环境 克隆Spark源码： $ git clone https://github.com/apache/spark 然后在IntelliJ IDEA中即可通过\"Import Project\"，选择sbt项目，选择\"Use auto-import\"，即可生成IntelliJ项目文件。","tags":"Blogs","title":"使用IntelliJ IDEA配置Spark应用开发环境及源码阅读环境"},{"url":"http://blog.tomgou.xyz/spark-160-dan-ji-an-zhuang-pei-zhi.html","text":"本文将介绍Apache Spark 1.6.0在单机的部署，与在集群中部署的步骤基本一致，只是少了一些master和slave文件的配置。直接安装scala与Spark就可以在单机使用，但如果用到hdfs系统的话hadoop和jdk也要配置，建议全部安装配置好。转载请注明博客原文地址： http://blog.tomgou.xyz/spark-160-dan-ji-an-zhuang-pei-zhi.html 0.Spark的安装准备 Spark官网的文档 http://spark.apache.org/docs/latest/ 里是这样说的： Spark runs on Java 7+, Python 2.6+ and R 3.1+. For the Scala API, Spark 1.6.0 uses Scala 2.10. You will need to use a compatible Scala version (2.10.x). 我的电脑环境是Ubuntu 14.04.4 LTS，还需要安装： jdk-8u73-linux-x64.tar.gz hadoop-2.6.0.tar.gz scala-2.10.6.tgz spark-1.6.0-bin-hadoop2.6.tgz 1.安装jdk 解压jdk安装包到任意目录： cd /home/tom $ tar -xzvf jdk-8u73-linux-x64.tar.gz $ sudo vim /etc/profile 编辑/etc/profile文件，在最后加上java环境变量： export JAVA_HOME=/home/tom/jdk1.8.0_73/ export JRE_HOME=/home/tom/jdk1.8.0_73/jre export PATH= $ JAVA_HOME /bin: $ JRE_HOME /bin: $ PATH export CLASSPATH=.: $ JAVA_HOME /lib: $ JRE_HOME /lib: $ CLASSPATH 保存并更新 /etc/profile ： $ source /etc/profil 查看是否成功： $ java -version 2.配置ssh localhost 确保安装好ssh： $ sudo apt-get update $ sudo apt-get install openssh-server $ sudo /etc/init.d/ssh start 生成并添加密钥： $ ssh-keygen -t rsa $ cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys $ chmod 0600 ~/.ssh/authorized_keys 如果已经生成过密钥，只需执行后两行命令。 测试ssh localhost $ ssh localhost $ exit 3.安装hadoop2.6.0 解压hadoop2.6.0到任意目录： $ cd /home/tom $ wget http://apache.claz.org/hadoop/common/hadoop-2.6.0/hadoop-2.6.0.tar.gz $ tar -xzvf hadoop-2.6.0.tar.gz 编辑 /etc/profile 文件，在最后加上java环境变量： export HADOOP_HOME=/home/tom/hadoop-2.6.0 export HADOOP_INSTALL= $ HADOOP_HOME export HADOOP_MAPRED_HOME= $ HADOOP_HOME export HADOOP_COMMON_HOME= $ HADOOP_HOME export HADOOP_HDFS_HOME= $ HADOOP_HOME export YARN_HOME= $ HADOOP_HOME export HADOOP_COMMON_LIB_NATIVE_DIR= $ HADOOP_HOME /lib/native export PATH= $ PATH : $ HADOOP_HOME /sbin: $ HADOOP_HOME /bin 编辑 $HADOOP_HOME/etc/hadoop/hadoop-env.sh 文件 $ vim $HADOOP_HOME/etc/hadoop/hadoop-env.sh 在最后加上： export JAVA_HOME=/home/tom/jdk1.8.0_73/ 修改Configuration文件： $ cd $HADOOP_HOME/etc/hadoop 修改 core-site.xml ： <configuration> <property> <name> fs.default.name </name> <value> hdfs://localhost:9000 </value> </property> </configuration> 修改 hdfs-site.xml ： <configuration> <property> <name> dfs.replication </name> <value> 1 </value> </property> <property> <name> dfs.name.dir </name> <value> file:///home/tom/hadoopdata/hdfs/namenode </value> </property> <property> <name> dfs.data.dir </name> <value> file:///home/tom/hadoopdata/hdfs/datanode </value> </property> </configuration> 第一个是dfs的备份数目，单机用1份就行，后面两个是namenode和datanode的目录。 修改 mapred-site.xml ： <configuration> <property> <name> mapreduce.framework.name </name> <value> yarn </value> </property> </configuration> 修改 yarn-site.xml ： <configuration> <property> <name> yarn.nodemanager.aux-services </name> <value> mapreduce_shuffle </value> </property> </configuration> 初始化hadoop： $ hdfs namenode -format 启动 $ $HADOOP_HOME/sbin/start-all.sh 停止 $ $HADOOP_HOME/sbin/stop-all.sh 检查WebUI，浏览器打开端口： http://localhost:8088 port 8088: cluster and all applications port 50070: Hadoop NameNode port 50090: Secondary NameNode port 50075: DataNode hadoop运行后可使用 jps 命令查看,得到结果： 10057 Jps 9611 ResourceManager 9451 SecondaryNameNode 9260 DataNode 9102 NameNode 9743 NodeManager 4.安装scala 解压scala安装包到任意目录： $ cd /home/tom $ tar -xzvf scala-2.10.6.tgz $ sudo vim /etc/profile 在 /etc/profile 文件的末尾添加环境变量： export SCALA_HOME=/home/tom//scala-2.10.6 export PATH= $ SCALA_HOME /bin: $ PATH 保存并更新 /etc/profile ： $ source /etc/profil 查看是否成功： $ scala -version 5.安装Spark 解压spark安装包到任意目录： $ cd /home/tom $ tar -xzvf spark-1.6.0-bin-hadoop2.6.tgz $ mv spark-1.6.0-bin-hadoop2.6 spark-1.6.0 $ sudo vim /etc/profile 在 /etc/profile 文件的末尾添加环境变量： export SPARK_HOME=/home/tom/spark-1.6.0 export PATH= $ SPARK_HOME /bin: $ PATH 保存并更新 /etc/profile ： $ source /etc/profil 在conf目录下复制并重命名 spark-env.sh.template 为 spark-env.sh ： $ cp spark-env.sh.template spark-env.sh $ vim spark-env.sh 在 spark-env.sh 中添加： export JAVA_HOME=/home/tom/jdk1.8.0_73/ export SCALA_HOME=/home/tom//scala-2.10.6 export SPARK_MASTER_IP=localhost export SPARK_WORKER_MEMORY=4G 启动 $ $SPARK_HOME/sbin/start-all.sh 停止 $ $SPARK_HOME/sbin/stop-all.sh 测试Spark是否安装成功： $ $SPARK_HOME/bin/run-example SparkPi 得到结果： Pi is roughly 3.14716 检查WebUI，浏览器打开端口： http://localhost:8080","tags":"Blogs","title":"Spark 1.6.0 单机安装配置"},{"url":"http://blog.tomgou.xyz/c-review1.html","text":"1.数据类型 type byte bit 有效数字 int 4bytes 32bits(-2&#94;31 ~ 2&#94;31-1) short 2bytes 16bits long 4bytes 32bits unsigned 4bytes 32bits(0～2&#94;32-1) bool 1byte char 1byte float 4bytes 32bits(-2&#94;128 ~ 2&#94;128) 6~7 double 8bytes 64bits(-2&#94;1024 ~ 2&#94;1024) 15~16 long double 16bytes 18~19 注意： long int 和 int在16位以上机器相同，均为32位，16位机器int为16位 bool值有两种true和false，字节值为1或0 char一个字节，字节中存放的是对应字符的ASCII值，如 'A' 实型数（浮点型）在计算机内不能精确表示。 其中float4字节，32位，符号位1位，指数位8位，于是float的指数范围为-127~128。float尾数部分23位，2&#94;23 = 8388608，一共七位，这意味着float最多能有7位有效数字，但绝对能保证的为6位。 可以通过 sizeof() 了解不同类型占用多少内存量（字节）： #include <iostream> int main () { std :: cout << sizeof ( float ) << std :: endl ; std :: cout << sizeof ( double ) << std :: endl ; std :: cout << sizeof ( long double ) << std :: endl ; return 0 ; } 执行结果： 4 8 16 123e3,123E3(科学计数法 123*10&#94;3 ) 一维数组 int a[10]; 二维数组 int a[4][5];//4*5 字符串 char ch[] = \"Hello,world\"; 或 char ch[] = {\"Hello,world\"}; 或 char ch[] = {'H','e','l','l','o',',','w','o','r','l','d','\\0'}; == 以 '\\0' 为结束符 == 'a' 与 \"a\" ： 'a' 为字符常量，一字节（a的ASCII值）； \"a\" 为字符串，2字节（a的ASCII值和'\\0'） 字符串处理函数 #include<cstring> strcpy(dst, src) strlen(s) strchr(s, ch) strcmp(s1, s2) enum枚举 enum weekdayT{SUnday, Monday}; 2.基础知识 位，比特，bit(二进制的一位) 字节，byte，B（8个位组成一个字节） 注释 //一行的注释 /*多行 的注释 */ 库包含 #include <iostream> #include \"user.h\" //自己写的库user 宏定义 #define PI 3.14159 转义 '\\n' 换行 '\\t' Tab '\\\\' 反斜杠 内存量 sizeof() 符号常量 const double PI = 3.1415926 数学函数库 #include <cmath> int abs ( int x ); double fabs ( double x ); double exp ( double x ); double sqrt ( double x ); 自增自减运算 y = x ++ ; //先 y = x，再x++ y = ++ x ; 输入输出 cin >> a ; cin . get (); cin . getline ( ch1 , 80 , '.' ); //字符串输入（,数组长度,结束标记） cout << a << endl ; 主程序 //#include <iostream> using namespace std ; //名字空间 int main () { cout << \"Hello world!\" << endl ; return 0 ; } 条件句 < 小于 <= 小于等于 == == 等于 == != 不等于 逻辑运算 && 与 || 或 ! 非 3.控制流 if语句 if ( ) XXX ; else { XXX ; XXX ; } 条件表达式?: max = ( x > y ) ? x : y ; switch语句 switch (){ case const1 : XXX ; break ; case const2 : XXX ; break ; default : XXX ; break ; } for循环 for ( i = 0 ; i < n ; ++ i ){ XXX ; } while循环 while ( ){ XXX ; } do-while循环 do { XXX ; } while ( ); 循环中途退出 while ( ){ XXX ; if ( ) break ; } 随机数 #include <cstdlib> #include <ctime> srand ( time ( NULL )); //随机种子初始化 num1 = rand () * 10 / ( RAND_MAX + 1 ); num2 = rand () * 4 / ( RAND_MAX + 1 );","tags":"Blogs","title":"C++ Review(1)"}]}