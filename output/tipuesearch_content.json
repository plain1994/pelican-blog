{"pages":[{"url":"http://blog.tomgou.xyz/kuai-xue-scalachu-ji-a1zheng-li-1.html","text":"Scala是一门有趣且实用的语言，它以JVM位目标环境，将面向对象和函数式编程有机地结合起来，带来独特的编程体验。本文主要整理《快学Scala》中的初级A1部分，适用于Scala的初级应用开发学习。转载请注明博客原文地址： http://blog.tomgou.xyz/kuai-xue-scalachu-ji-a1zheng-li-1.html 0.安装和运行scala 解压scala安装包到任意目录： $ cd /home/tom $ tar -xzvf scala-2.10.6.tgz $ sudo vim /etc/profile 在 /etc/profile 文件的末尾添加环境变量： export SCALA_HOME=/home/tom//scala-2.10.6 export PATH= $ SCALA_HOME /bin: $ PATH 保存并更新 /etc/profile ： $ source /etc/profil 查看是否成功： $ scala -version 建立一个简单的scala程序 scalatest.scala ： object scalatest { def main ( args : Array [ String ]){ def box ( s : String ){ var border = \"-\" * s . length + \"--\\n\" println ( border + \"|\" + s + \"|\\n\" + border ) } box ( \"Hello Tom!\" ) } } 编译程序： $ scalac scalatest.scala 执行程序： $ scala scalatest 得到结果： ------------ |Hello Tom!| ------------ 1.Scala基础 1.1 Scala解释器 Scala解释器读到一个表达式，对它求值，将其打印，接着再继续读下一个表达式。这个过程被称为读取-求值-打印-循环，即REPL。 scala> 8 * 5 + 2 res0: Int = 42 从技术上讲，scala程序并不是一个解释器。实际发生的过程是，你输入的内容被快速地编译成字节码，然后这段字节码交由Java虚拟机执行。 REPL在同一时间只能看到一行代码。 在REPL粘贴成块代码，需要键入 :paste ，粘贴代码块，然后按下Ctrl+D。 1.2 常量与变量 以val定义常量，以var定义变量： val answer = 8 * 5 + 2 var counter = 0 counter = 1 在Scala中，我们鼓励使用val。同时，你不需要给出值或者变量的类型，这个信息可以从你初始化它的表达式中判断出来。必要时，可指定类型，如： val greeting : String = null 在Scala中，无需分号，仅当同一行代码中存在多条语句时才用分号分开。 1.3 常用类型 和Java一样，Scala有7种数据类型： Byte Char Short Int Long Float Double Boolean 跟Java不同的是，这些类型是类。你可以对数字执行方法，如： 1. toString () //产出字符串\"1\" 1. to ( 5 ) //产出Range(1,2,3,4,5) Scala用底层的java.lang.String类来表示字符串，不过通过StringOps类给字符串追加了上百种操作，比如： \"Hello\" . intersect ( \"World\" ) //输出\"lo\" Scala还提供了RichInt，RichDouble，RichChar等，提供了Int，Double，Char不具备的便捷方法。比如 1.to(10) 中，Int值1首先被转换成RichInt，然后调用to方法。 1.4 算术和操作符重载 +-*/%等操作符实际上是方法，比如 a+b 是如下方法 a.+(b) 的简写。 通常你可以用 a 方法 b 来简写方法 a.方法(b) 。比如 1.to(5) 可以写成 1 to 5 。 Scala没有提供 ++ 和 -- 操作符，需要使用 +=1 或 -=1 ，如 counter += 1 。 1.5 调用函数和方法 在Scala中使用数学函数比Java简单，你不需要从某个类调用它的静态方法。 sqrt ( 2 ) pow ( 2 , 4 ) //16.0 min ( 3 , Pi ) //3.0 这些数学函数在 scala.math 包中定义，可以通过以下语句引入： import scala.math._ //在Scala中，_字符是通配符，类似Java中的* 引入以scala.开头的包时，可以省去scala前缀。 不带参数的Scala方法通常不使用圆括号，如： \"Hello\" . distinct 1.6 apply方法 \"Hello\" ( 4 ) //将产出'o' \"Hello\"(4) 是如下方法的简写： \"Hello\" . apply ( 4 ) () 操作符可以作为apply方法的重载形式，比如说将字符串或数字转换成BigInt对象的apply方法, BigInt(\"123\") 是 BigInt.apply(\"123\") 的简写， Array(1,4,9) 返回一个数组，用的就是Array伴生对象的apply方法。 1.7 Scaladoc 可以在 http://www.scala-lang.org/api 在线浏览Scaladoc。 - 类名旁的O和C分别链接到对应的类（C）和伴生对象（O） - 标记为implicit的方法对应的是自动隐式转换 - 方法可以以函数作为参数 2.控制流与函数 2.1 条件表达式 Scala中的if/else表达式有值，这个值就是if或else后的值，如： val s = if ( x > 0 ) 1 else - 1 上式将if/else表达式的值赋值给常量。还可以这样写： if ( x > 0 ) s = 1 else s = - 1 不过第一种写法更好，第二种写法中，s必须是变量var。 如下是混合类型的条件表达式： if ( x > 0 ) \"positive\" else - 1 一个分支是String，另一个是Int，它们的公共超类型叫做Any。 当if语句没有输出值时，引入了Unit类，写做 () 。如下： if ( x > 0 ) 1 else () () 可以当做是表示\"无有用值\"的占位符。 2.2 语句终止 Scala与JavaScript等脚本语言类似，行尾位置不需要分号。 如果你在写较长的语句，需要分两行来写的话，确保第一行以一个不能用作语句结尾的符号结尾，如： s = s0 + ( v - v0 ) * t + 0.5 * ( a - a0 ) * t * t Scala程序员倾向使用Kernighan & Ritchie风格的花括号： if ( n > 0 ) { r = r * n n -= 1 } 2.3 块表达式与赋值 在Scala中， {} 块包含一系列表达式。块中最后一个表达式的值就是块的值。如： val distance = { val dx = x - x0 ; val dy = y - y0 ; sqrt ( dx * dx + dy * dy )} 赋值动作本身是没有值的，是Unit类型的。一个以赋值语句结束的块，比如： {r=r*n;n-=1} 的值是Unit类型的。 x=y=1//别这样 中，x被赋的值是Unit类型的。 2.4 输入与输出 打印： print ( \"hello\" ) println ( 42 ) //打印内容后会追加一个换行符 格式化字符串的printf： printf ( \" %s is %d years old.\\n\" , \"Fred\" , 42 ) 用readLine函数从控制台读取一行输入： val name = readLine ( \"Your name: \" ) 2.5 循环 Scala拥有与Java和C++相同的while和do循环，如： while ( n > 0 ){ r = r * n n -= 1 } for循环使用 for(i <- 表达式) 这样的语法结构，让变量i遍历表达式的所有值，如： for ( i <- 1 to n ) r = r * i 遍历字符串或数组时，通常需要使用0到n-1的区间，until方法返回一个不包含上限的区间： val s = \"hello\" var sum = 0 for ( i <- 0 until s . length ) sum += s ( i ) 或： var sum = 0 for ( ch <- 0 \"hello\" ) sum += ch Scala没有提供break或continue语句来退出循环，我们可以： 1.使用Boolean型的控制变量。 2.嵌套函数，函数中return。 3.使用Breaks对象中的break方法。 2.6 高级for循环 多个生成器： for ( i <- 1 to 3 ; j <- 1 to 3 ) print (( 10 * i + j )+ \" \" ) //打印11 12 13 21 22 23 31 32 33 生成器可带守卫（注意：if前无分号）： for ( i <- 1 to 3 ; j <- 1 to 3 if i != j ) print (( 10 * i + j )+ \" \" ) //打印12 13 21 23 31 32 for推导式（yield），循环会构造出一个集合，集合与第一个生成器的类型兼容： for ( i <- 1 to 10 ) yield i % 3 //生成Vector(1,2,0,1,2,0,1,2,0,1) for ( c <- \"Hello\" ; i <- 0 to 1 ) yield ( c + i ). toChar //生成\"HIeflmlmop\" for ( i <- 0 to 1 ; c <- \"Hel\" ) yield ( c + i ). toChar //生成Vector('H','e','l','I','f','m') 2.7 函数 定义函数需要名称，参数，函数体： def abs ( x : Double ) = if ( x >= 0 ) x else - x 函数可以使用代码块，块中最后一个表达式的值是函数的返回值： def fac ( n : Int ) = { var r = 1 for ( i <- 1 to n ) r = r * i r } 对于递归函数，必须指明返回类型： def fac ( n : Int ) : Int = if ( n <= 0 ) 1 else n * fac ( n - 1 ) 2.8 过程 过程就是不返回值的函数，返回类型为Unit。定义时可以略去'='，如： def box ( s : String ){ var border = \"-\" * s . length + \"--\\n\" println ( border + \"|\" + s + \"|\\n\" + border ) } 也可以显示声明： def box ( s : String ) : Unit = { var border = \"-\" * s . length + \"--\\n\" println ( border + \"|\" + s + \"|\\n\" + border ) } 3.数组 3.1 定长数组Array val nums = new Array [ Int ]( 10 ) //10个整数的数组，所有元素初始化为0 val a = new Array [ String ]( 10 ) //10个元素的字符串数组，所有元素初始化为null val s = Array ( \"Hello\" , \"World\" ) //已提供初始值时不需要new s ( 0 ) = \"Goodbye\" //使用()来访问元素 在JVM中，Scala的Array以Java数组实现，比如 Array(2,3,5,7,11) 在JVM中就是一个 Int[] 。 3.2 变长数组ArrayBuffer 对于长度按需变化的数组，Java有ArrayList，C++有vector，Scala中的等效数据结构为ArrayBuffer。 val b = ArrayBuffer [ Int ]() //或val b = new ArrayBuffer[Int] b += 1 //用+=在尾端添加元素 b += ( 1 , 2 , 3 , 5 ) //添加多个元素 b ++= Array ( 8 , 13 , 21 ) //用++=追加任何集合 b . trimEnd ( 5 ) //移除最后5个元素 有时需要构建一个Array，但不知道最终要装多少元素，可以先构建一个数组缓冲ArrayBuffer，然后转化为Array： b . toArray 同时，你可以使用 insert 和 remove 来插入或移除元素。 3.3 遍历数组 使用for循环： for ( i <- 0 until a . length ) println ( i + \": \" + a ( i )) until是RichInt类的方法， 0 until 10 实际上是 0.until(10) ，如果想让区间是每两个元素一跳，可以这样写 0 until (a.length, 2) ，如果从数组尾端开始，可以这样写 (0 until a.length).reverse 。 如果在循环体中不需要用到数组下标，我们可以直接访问数组元素： for ( elem <- a ) println ( elem ) 3.4 数组转换 在Scala，从一个数组或变长数组出发，以某种方式对它进行转换是很简单的，这些转换动作不会修改原始数组，而是产生一个全新的数组。 比如使用for yield推倒式： val a = Array ( 2 , 3 , 5 , 7 , 11 ) val result = for ( elem <- a ) yield 2 * elem //result是Array(4,6,10,14,22) 当你遍历一个集合，只想处理那些满足特定条件的元素时，你可以加入守卫,比如这里对每个偶数元素翻倍，丢到奇数元素： for ( elem <- a if elem % 2 == 0 ) yield 2 * elem 另一种做法： a . filter ( _ % 2 == 0 ). map ( 2 * _ ) 3.5 数组常用算法 求最小和最大 min 和 max 求和（数值类型）： Array ( 1 , 7 , 2 , 9 ). sum //19 排序 sorted ： val b = ArrayBuffer ( 1 , 7 , 2 , 9 ) val bSorted = b . sorted //ArrayBuffer(1,2,7,9) 排序（提供比较函数） sortWwith ： val bDescending = b . sortWith ( _ > _ ) //ArrayBuffer(9,7,2,1) 显示 mkString ： b . mkString ( \" and \" ) //\"1 and 7 and 2 and 9\" 显示 toString ： b . toString //\"ArrayBuffer(1,7,2,9)\" 3.6 多维数组 多维数组是通过数组的数组来实现的，举例来说， Double 的二维数组类型为 Array[Array[Double]] ，构造时可以用ofDim方法： val matrix = Array . ofDim [ Double ]( 3 , 4 ) //3*4 访问元素时使用两对圆括号： matrix ( row )( column ) = 42 参考书籍： 快学Scala","tags":"Blogs","title":"快学Scala初级A1整理(1)"},{"url":"http://blog.tomgou.xyz/shi-yong-pycharmpei-zhi-sparkde-pythonkai-fa-huan-jing.html","text":"在本地搭建好Spark 1.6.0后，除了使用spark-submit提交Python程序外，我们可以使用PyCharm这个IDE在本地进行开发调试,提升我们的开发效率。配置过程也十分简单，在stackoverflow上搜索到的。同时，IntelliJ IDEA加入Python插件后也可以使用Python开发Spark程序，配置步骤一致。转载请注明博客原文地址： http://blog.tomgou.xyz/shi-yong-pycharmpei-zhi-sparkde-pythonkai-fa-huan-jing.html 0.安装PyCharm和py4j 我的系统环境（Ubuntu 14.04.4 LTS） 下载安装最新版本的PyCharm，官网地址： https://www.jetbrains.com/pycharm/download/ 。 安装步骤： Unpack the pycharm-5.0.4.tar.gz using the following command: tar xfz pycharm-5.0.4.tar.gz Run pycharm.sh from the bin subdirectory 安装py4j： $ sudo pip install py4j 1.配置Pycharm 打开PyCharm，创建一个Project。 然后选择\"Run\" ->\"Edit Configurations\" ->\"Environment variables\" 增加SPARK_HOME目录与PYTHONPATH目录。 - SPARK_HOME:Spark安装目录 - PYTHONPATH:Spark安装目录下的Python目录 2.测试Pycharm 运行一个小的Spark程序看看： \"\"\"SimpleApp\"\"\" from pyspark import SparkContext logFile = \"/home/tom/spark-1.6.0/README.md\" sc = SparkContext ( \"local\" , \"Simple App\" ) logData = sc . textFile ( logFile ) . cache () numAs = logData . filter ( lambda s : 'a' in s ) . count () numBs = logData . filter ( lambda s : 'b' in s ) . count () print ( \"Lines with a: %i , lines with b: %i \" % ( numAs , numBs )) 运行结果： Lines with a: 58, lines with b: 26","tags":"Blogs","title":"使用PyCharm配置Spark的Python开发环境"},{"url":"http://blog.tomgou.xyz/shi-yong-intellij-ideapei-zhi-sparkying-yong-kai-fa-huan-jing-ji-yuan-ma-yue-du-huan-jing.html","text":"在本地搭建好Spark 1.6.0后，除了使用官方文档中的sbt命令打包，spark-submit提交程序外，我们可以使用IntelliJ IDEA这个IDE在本地进行开发调试，之后再将作业提交到集群生产环境中运行，使用IDE可以提升我们的开发效率。转载请注明博客原文地址： http://blog.tomgou.xyz/shi-yong-intellij-ideapei-zhi-sparkying-yong-kai-fa-huan-jing-ji-yuan-ma-yue-du-huan-jing.html 0.安装IntelliJ IDEA 我的系统环境（Ubuntu 14.04.4 LTS） 下载最新版本的IntelliJ IDEA，官网地址： https://www.jetbrains.com/idea/download/ 。 最新版本的IntelliJ IDEA支持新建SBT工程，安装scala插件。 安装步骤： Unpack the idea idea-15.0.4.tar.gz file using the following command: tar xfz idea-15.0.4.tar.gz Run idea.sh from the bin subdirectory. 记得在IntelliJ IDEA的\"Configure\"菜单中，选择\"Plugins\"，安装\"Scala\"插件。 1.以本地local模式运行Spark程序 1）创建\"New Project\"，选择\"Scala\"。\"Project SDK\"选择JDK目录，\"Scala SDK\"选择Scala目录。 2）选择菜单中的\"File\" ->\"Project Structure\" ->\"libraries\" ->+\"java\"，导入Spark安装目录 /home/tom/spark-1.6.0/lib 下的\" spark-assembly-1.6.0-hadoop2.6.0.jar \"。 3）运行Scala示例程序SparkPi： Spark安装目录的examples目录下，可以找到Scala编写的示例程序 SparkPi.scala ，该程序计算Pi值并输出。 在Project的main目录下新建 SparkPitest.scala ，复制Spark示例程序代码。 选择菜单中的\"Run\" ->\"Edit Configurations\"，修改\"Main class\"和\"VM options\"。 运行结果： 注意： 在我最初运行Spark的测试程序SparkPi时，点击运行，出现了如下错误： Exception in thread \"main\" org.apache.spark.SparkException: A master URL must be set in your configuration 从提示中可以看出找不到程序运行的master，此时需要配置环境变量。 搜索引擎查询错误后，了解到传递给spark的master url可以有如下几种，具体可以查看Spark官方文档： local 本地单线程 local[K] 本地多线程（指定K个内核） local[*] 本地多线程（指定所有可用内核） spark://HOST:PORT 连接到指定的 Spark standalone cluster master，需要指定端口。 mesos://HOST:PORT 连接到指定的 Mesos 集群，需要指定端口。 yarn-client客户端模式 连接到 YARN 集群。需要配置 HADOOP_CONF_DIR。 yarn-cluster集群模式 连接到 YARN 集群。需要配置 HADOOP_CONF_DIR。 在VM options中输入\" -Dspark.master=local \"，指示本程序本地单线程运行。 2.生成jar包提交到集群 1）和本地local模式运行Spark相同，我们建立起project。 2）选择菜单中的\"File\" ->\"Project Structure\" ->\"Artifact\" ->\"jar\" ->\"From Modules with dependencies\"，之后选择Main Class和输出jar的Directory。 3）在主菜单选择\"Build\" ->\"Build Artifact\"，编译生成jar包。 4）将jar包使用 spark-submit 提交： $SPARK_HOME/bin/spark-submit --class \"SimpleApp\" --master local [ 4 ] simple.jar 3.配置Spark源码阅读环境 克隆Spark源码： $ git clone https://github.com/apache/spark 然后在IntelliJ IDEA中即可通过\"Import Project\"，选择sbt项目，选择\"Use auto-import\"，即可生成IntelliJ项目文件。","tags":"Blogs","title":"使用IntelliJ IDEA配置Spark应用开发环境及源码阅读环境"},{"url":"http://blog.tomgou.xyz/spark-160-dan-ji-an-zhuang-pei-zhi.html","text":"本文将介绍Apache Spark 1.6.0在单机的部署，与在集群中部署的步骤基本一致，只是少了一些master和slave文件的配置。直接安装scala与Spark就可以在单机使用，但如果用到hdfs系统的话hadoop和jdk也要配置，建议全部安装配置好。转载请注明博客原文地址： http://blog.tomgou.xyz/spark-160-dan-ji-an-zhuang-pei-zhi.html 0.Spark的安装准备 Spark官网的文档 http://spark.apache.org/docs/latest/ 里是这样说的： Spark runs on Java 7+, Python 2.6+ and R 3.1+. For the Scala API, Spark 1.6.0 uses Scala 2.10. You will need to use a compatible Scala version (2.10.x). 我的电脑环境是Ubuntu 14.04.4 LTS，还需要安装： jdk-8u73-linux-x64.tar.gz hadoop-2.6.0.tar.gz scala-2.10.6.tgz spark-1.6.0-bin-hadoop2.6.tgz 1.安装jdk 解压jdk安装包到任意目录： cd /home/tom $ tar -xzvf jdk-8u73-linux-x64.tar.gz $ sudo vim /etc/profile 编辑/etc/profile文件，在最后加上java环境变量： export JAVA_HOME=/home/tom/jdk1.8.0_73/ export JRE_HOME=/home/tom/jdk1.8.0_73/jre export PATH= $ JAVA_HOME /bin: $ JRE_HOME /bin: $ PATH export CLASSPATH=.: $ JAVA_HOME /lib: $ JRE_HOME /lib: $ CLASSPATH 保存并更新 /etc/profile ： $ source /etc/profil 查看是否成功： $ java -version 2.配置ssh localhost 确保安装好ssh： $ sudo apt-get update $ sudo apt-get install openssh-server $ sudo /etc/init.d/ssh start 生成并添加密钥： $ ssh-keygen -t rsa $ cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys $ chmod 0600 ~/.ssh/authorized_keys 如果已经生成过密钥，只需执行后两行命令。 测试ssh localhost $ ssh localhost $ exit 3.安装hadoop2.6.0 解压hadoop2.6.0到任意目录： $ cd /home/tom $ wget http://apache.claz.org/hadoop/common/hadoop-2.6.0/hadoop-2.6.0.tar.gz $ tar -xzvf hadoop-2.6.0.tar.gz 编辑 /etc/profile 文件，在最后加上java环境变量： export HADOOP_HOME=/home/tom/hadoop-2.6.0 export HADOOP_INSTALL= $ HADOOP_HOME export HADOOP_MAPRED_HOME= $ HADOOP_HOME export HADOOP_COMMON_HOME= $ HADOOP_HOME export HADOOP_HDFS_HOME= $ HADOOP_HOME export YARN_HOME= $ HADOOP_HOME export HADOOP_COMMON_LIB_NATIVE_DIR= $ HADOOP_HOME /lib/native export PATH= $ PATH : $ HADOOP_HOME /sbin: $ HADOOP_HOME /bin 编辑 $HADOOP_HOME/etc/hadoop/hadoop-env.sh 文件 $ vim $HADOOP_HOME/etc/hadoop/hadoop-env.sh 在最后加上： export JAVA_HOME=/home/tom/jdk1.8.0_73/ 修改Configuration文件： $ cd $HADOOP_HOME/etc/hadoop 修改 core-site.xml ： <configuration> <property> <name> fs.default.name </name> <value> hdfs://localhost:9000 </value> </property> </configuration> 修改 hdfs-site.xml ： <configuration> <property> <name> dfs.replication </name> <value> 1 </value> </property> <property> <name> dfs.name.dir </name> <value> file:///home/tom/hadoopdata/hdfs/namenode </value> </property> <property> <name> dfs.data.dir </name> <value> file:///home/tom/hadoopdata/hdfs/datanode </value> </property> </configuration> 第一个是dfs的备份数目，单机用1份就行，后面两个是namenode和datanode的目录。 修改 mapred-site.xml ： <configuration> <property> <name> mapreduce.framework.name </name> <value> yarn </value> </property> </configuration> 修改 yarn-site.xml ： <configuration> <property> <name> yarn.nodemanager.aux-services </name> <value> mapreduce_shuffle </value> </property> </configuration> 初始化hadoop： $ hdfs namenode -format 启动 $ $HADOOP_HOME/sbin/start-all.sh 停止 $ $HADOOP_HOME/sbin/stop-all.sh 检查WebUI，浏览器打开端口： http://localhost:8088 port 8088: cluster and all applications port 50070: Hadoop NameNode port 50090: Secondary NameNode port 50075: DataNode hadoop运行后可使用 jps 命令查看,得到结果： 10057 Jps 9611 ResourceManager 9451 SecondaryNameNode 9260 DataNode 9102 NameNode 9743 NodeManager 4.安装scala 解压scala安装包到任意目录： $ cd /home/tom $ tar -xzvf scala-2.10.6.tgz $ sudo vim /etc/profile 在 /etc/profile 文件的末尾添加环境变量： export SCALA_HOME=/home/tom//scala-2.10.6 export PATH= $ SCALA_HOME /bin: $ PATH 保存并更新 /etc/profile ： $ source /etc/profil 查看是否成功： $ scala -version 5.安装Spark 解压spark安装包到任意目录： $ cd /home/tom $ tar -xzvf spark-1.6.0-bin-hadoop2.6.tgz $ mv spark-1.6.0-bin-hadoop2.6 spark-1.6.0 $ sudo vim /etc/profile 在 /etc/profile 文件的末尾添加环境变量： export SPARK_HOME=/home/tom/spark-1.6.0 export PATH= $ SPARK_HOME /bin: $ PATH 保存并更新 /etc/profile ： $ source /etc/profil 在conf目录下复制并重命名 spark-env.sh.template 为 spark-env.sh ： $ cp spark-env.sh.template spark-env.sh $ vim spark-env.sh 在 spark-env.sh 中添加： export JAVA_HOME=/home/tom/jdk1.8.0_73/ export SCALA_HOME=/home/tom//scala-2.10.6 export SPARK_MASTER_IP=localhost export SPARK_WORKER_MEMORY=4G 启动 $ $SPARK_HOME/sbin/start-all.sh 停止 $ $SPARK_HOME/sbin/stop-all.sh 测试Spark是否安装成功： $ $SPARK_HOME/bin/run-example SparkPi 得到结果： Pi is roughly 3.14716 检查WebUI，浏览器打开端口： http://localhost:8080","tags":"Blogs","title":"Spark 1.6.0 单机安装配置"},{"url":"http://blog.tomgou.xyz/c-review1.html","text":"1.数据类型 type byte bit 有效数字 int 4bytes 32bits(-2&#94;31 ~ 2&#94;31-1) short 2bytes 16bits long 4bytes 32bits unsigned 4bytes 32bits(0～2&#94;32-1) bool 1byte char 1byte float 4bytes 32bits(-2&#94;128 ~ 2&#94;128) 6~7 double 8bytes 64bits(-2&#94;1024 ~ 2&#94;1024) 15~16 long double 16bytes 18~19 注意： long int 和 int在16位以上机器相同，均为32位，16位机器int为16位 bool值有两种true和false，字节值为1或0 char一个字节，字节中存放的是对应字符的ASCII值，如 'A' 实型数（浮点型）在计算机内不能精确表示。 其中float4字节，32位，符号位1位，指数位8位，于是float的指数范围为-127~128。float尾数部分23位，2&#94;23 = 8388608，一共七位，这意味着float最多能有7位有效数字，但绝对能保证的为6位。 可以通过 sizeof() 了解不同类型占用多少内存量（字节）： #include <iostream> int main () { std :: cout << sizeof ( float ) << std :: endl ; std :: cout << sizeof ( double ) << std :: endl ; std :: cout << sizeof ( long double ) << std :: endl ; return 0 ; } 执行结果： 4 8 16 123e3,123E3(科学计数法 123*10&#94;3 ) 一维数组 int a[10]; 二维数组 int a[4][5];//4*5 字符串 char ch[] = \"Hello,world\"; 或 char ch[] = {\"Hello,world\"}; 或 char ch[] = {'H','e','l','l','o',',','w','o','r','l','d','\\0'}; == 以 '\\0' 为结束符 == 'a' 与 \"a\" ： 'a' 为字符常量，一字节（a的ASCII值）； \"a\" 为字符串，2字节（a的ASCII值和'\\0'） 字符串处理函数 #include<cstring> strcpy(dst, src) strlen(s) strchr(s, ch) strcmp(s1, s2) enum枚举 enum weekdayT{SUnday, Monday}; 2.基础知识 位，比特，bit(二进制的一位) 字节，byte，B（8个位组成一个字节） 注释 //一行的注释 /*多行 的注释 */ 库包含 #include <iostream> #include \"user.h\" //自己写的库user 宏定义 #define PI 3.14159 转义 '\\n' 换行 '\\t' Tab '\\\\' 反斜杠 内存量 sizeof() 符号常量 const double PI = 3.1415926 数学函数库 #include <cmath> int abs ( int x ); double fabs ( double x ); double exp ( double x ); double sqrt ( double x ); 自增自减运算 y = x ++ ; //先 y = x，再x++ y = ++ x ; 输入输出 cin >> a ; cin . get (); cin . getline ( ch1 , 80 , '.' ); //字符串输入（,数组长度,结束标记） cout << a << endl ; 主程序 //#include <iostream> using namespace std ; //名字空间 int main () { cout << \"Hello world!\" << endl ; return 0 ; } 条件句 < 小于 <= 小于等于 == == 等于 == != 不等于 逻辑运算 && 与 || 或 ! 非 3.控制流 if语句 if ( ) XXX ; else { XXX ; XXX ; } 条件表达式?: max = ( x > y ) ? x : y ; switch语句 switch (){ case const1 : XXX ; break ; case const2 : XXX ; break ; default : XXX ; break ; } for循环 for ( i = 0 ; i < n ; ++ i ){ XXX ; } while循环 while ( ){ XXX ; } do-while循环 do { XXX ; } while ( ); 循环中途退出 while ( ){ XXX ; if ( ) break ; } 随机数 #include <cstdlib> #include <ctime> srand ( time ( NULL )); //随机种子初始化 num1 = rand () * 10 / ( RAND_MAX + 1 ); num2 = rand () * 4 / ( RAND_MAX + 1 );","tags":"Blogs","title":"C++ Review(1)"}]}